#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Processor to take dh data, average by epoch and apply GIA correction

Takes altimetry single-mission gridded dh and time files generated by surfacefit.py in .npz format


Example of usage (using input directory option and otherwise default settings):

epoch_average.py -g /cpnet/altimetry/landice/sec_processing/single_mission_scenarios/s3b/surfacefit_latest

Output of example would be:

/cpnet/altimetry/landice/sec_processing/single_mission_scenarios/s3b/surfacefit_latest/epoch_average_s3b_antarctica.npz   
   npz file containing numpy grids of dh, etc
/cpnet/altimetry/landice/sec_processing/single_mission_scenarios/s3b/surfacefit_latest/epoch_average_s3b_antarctica.info   
   text file containing all settings and stats


Example of usage (using input file options and overriding all defaults):

epoch_average.py \
-f1 /cpnet/altimetry/landice/sec_processing/s3a_test/surfacefit_s3a_antarctica_07122016_04072020.dh.npz  \
-f2 /cpnet/altimetry/landice/sec_processing/s3a_test/surfacefit_s3a_antarctica_07122016_04072020.time.npz  \
-o /raid6/tmp \
-e 140 \
-t1 2017.0 \
-t2 2020.0 \
-gia ij05

Output of example would be:
/raid6/tmp/epoch_average_s3a_antarctica_07122016_04072020.npz
   npz file containing numpy grids of dh etc
/raid6/tmp/epoch_average_s3a_antarctica_07122016_04072020.nfo
   text file containing all settings and stats

Output npz file contents:
dh_ave - sigma-clipped mean dh, in m, arranged as epoch by grid y by grid x
input_dh_dens - number of datapoints that went into dh_ave, arranged ditto
input_dh_stddev - population standard deviation datapoints that went into dh_ave, in m, arranged ditto
input_dh_start_time - time of earliest datapoint that went into dh_ave, in years past 1991.0, arranged ditto
input_dh_end_time - time of latest datapoint that went into dh_ave, in years past 1991.0, arranged ditto
epoch_lo - lower epoch time limits, in years past 1991.0, as a 1d array
epoch_hi - upper epoch time limits, in years past 1991.0, as a 1d array
epoch_lo_dt - lower epoch time limits, as datetime objects, as a 1d array
epoch_hi_dt - upper epoch time limits, as datetime objects, as a 1d array

Note in any one epoch/cell epoch_lo <= datapoint times < epoch_hi.


Author: Lin Gilbert (CPOM/UCL)
Date: 2020
Copyright: UCL/MSSL/CPOM. Not to be used outside CPOM/MSSL without permission of author 

"""

# -------------------------------------------------------------------------------------------------------------
# Classes  (useful classes for this program)
#   all use os, so import here
# -------------------------------------------------------------------------------------------------------------

import os
import sys
import logging
import resource

log=logging.getLogger(__name__)


def epoch_average(griddir=None,
                  dhfile=None,
                  timefile=None,
                  outdir=None,
                  epoch_length=None,
                  epoch_start=None,
                  epoch_end=None,
                  gia_model=None):
    # ----------------------------------------------------------------------
    #  Import python modules
    # ----------------------------------------------------------------------

    import sys
    import glob
    import math
    import time
    import numpy as np
    import numpy.ma as ma
    from timeit import default_timer as timer
    from datetime import datetime, timedelta
    from calendar import isleap
    import array as array
    import git
    from astropy.stats import sigma_clipped_stats  # Robust stats

    # ----------------------------------------------------------------------
    #  Import CPOM python modules
    # ----------------------------------------------------------------------

    from cpom.gia_models.gia_models import GIAs
    from cpom.gridding.gridareas import GridArea
    from cpom.altimetry.tools.surfacefit import SurfacefitInfo

    # -------------------------------------------------------------------------------------------------------------
    # Start processing
    # -------------------------------------------------------------------------------------------------------------

    print('------------------------------------------------')
    print('Epoch averaging')
    print('------------------------------------------------')

    start_time = datetime.now()

    # -------------------------------------------------------------------------------------------------------------
    # Check Command Line Arguments
    # -------------------------------------------------------------------------------------------------------------

    if griddir is not None and (dhfile is not None or timefile is not None):
        print('If --griddir is specified, do not specify --dhfile and --timefile, or vv.')
        print('Note --griddir is for specifying a directory containing input files')
        print('Note --dhfile and --timefile are for specifying input files themselves')
        sys.exit()

    if (dhfile is None or timefile is None) and griddir is None:
        sys.exit('missing command line argument(s) : either --griddir or --dhfile and --timefile must be included')

    if griddir:
        if not os.path.exists(griddir):
            sys.exit('--griddir path ({}) does not exist'.format(griddir))

    if outdir is None:
        print('No separate output directory specified')
        if dhfile:
            outdir = os.path.split(dhfile)[0]
        else:
            outdir = griddir
        print('Output directory set to same as input grid files: ', outdir)

    # Set default 30 day epoch length
    if epoch_length is None:
        epoch_length = '30'

    # Set default epoch start time to 1991.0
    if epoch_start is None:
        epoch_start = '1991.0'

    # -------------------------------------------------------------------------------------------------------------
    # Check output directory
    # -------------------------------------------------------------------------------------------------------------

    if not os.path.exists(outdir):
        try:
            os.makedirs(outdir)
        except:
            log.error('Can not create output directory {}'.format(outdir))
            raise OSError("Can't create output directory (%s)!" % (outdir))

    # -------------------------------------------------------------------------------------------------------------
    # Report on settings used
    # -------------------------------------------------------------------------------------------------------------

    print('epoch_length=', epoch_length)
    print('epoch_start=', epoch_start)
    if epoch_end is not None:
        print('epoch_end=', epoch_end)
    else:
        print('epochs will end at time of last datapoint, rounded up to a full epoch')
    if gia_model is not None:
        print('GIA correction will be applied, using model ', gia_model)
    else:
        print('No GIA correction will be applied')

    # -------------------------------------------------------------------------------------------------------------
    # Retrieve GIT commit hash code (uniquely identifies software commit version)
    # This commit version can be retrieved at a later date using 'git checkout commit_hash'
    # -------------------------------------------------------------------------------------------------------------

    repo = git.Repo(os.environ['CPOM_SOFTWARE_DIR'])
    git_commit_sha = repo.head.object.hexsha
    git_time_of_commit = time.gmtime(repo.head.commit.committed_date)
    git_day = git_time_of_commit.tm_mday
    git_month = git_time_of_commit.tm_mon
    git_year = git_time_of_commit.tm_year

    print('GIT commit software version :', git_commit_sha)
    print('GIT commit date : {}/{}/{}'.format(git_day, git_month, git_year))

    # -------------------------------------------------------------------------------------------------------------
    # Find the dh and time files (.npz format) to use.
    # -------------------------------------------------------------------------------------------------------------

    if griddir:

        dhfiles = glob.glob(griddir + '/*.dh.npz')
        n_dhfiles = len(dhfiles)
        if n_dhfiles == 1:
            dhfile = dhfiles[0]
        elif n_dhfiles == 0:
            log.error('No *.dh.npz files found in {}'.format(griddir))
            sys.exit('No *.dh.npz files found in {}'.format(griddir))
        else:
            log.error('More than one *.dh.npz file found in {}'.format(griddir))
            sys.exit('More than one *.dh.npz file found in {}. Use --dhfile argument instead.'.format(griddir))

        timefiles = glob.glob(griddir + '/*.time.npz')
        n_timefiles = len(timefiles)
        if n_timefiles == 1:
            timefile = timefiles[0]
        elif n_gridfiles == 0:
            log.error('No *.time.npz files found in {}'.format(griddir))
            sys.exit('No *.time.npz files found in {}'.format(griddir))
        else:
            log.error('More than one *.time.npz file found in {}'.format(griddir))
            sys.exit('More than one *.time.npz file found in {}. Use --timefile argument instead.'.format(griddir))

    # -------------------------------------------------------------------------------------------------------------
    # Load dh and time grids from file.
    # -------------------------------------------------------------------------------------------------------------

    if os.path.splitext(dhfile)[1] != '.npz':
        log.error(dhfile + ' must have .npz extension')
        sys.exit(dhfile + ' must have .npz extension')

    if not os.path.exists(dhfile):
        log.error(dhfile + ' does not exist')
        sys.exit(dhfile + ' does not exist')

    log.info('Loading dh grid from {}...'.format(dhfile))
    print('Loading dh grid from {}...'.format(dhfile))

    this_dh = np.load(dhfile, allow_pickle=True)

    dh_grid = this_dh.get('dh_grid')

    if os.path.splitext(timefile)[1] != '.npz':
        log.error(timefile + ' must have .npz extension')
        sys.exit(timefile + ' must have .npz extension')

    if not os.path.exists(timefile):
        log.error(timefile + ' does not exist')
        sys.exit(timefile + ' does not exist')

    log.info('Loading time grid from {}...'.format(timefile))
    print('Loading time grid from {}...'.format(timefile))

    this_time = np.load(timefile, allow_pickle=True)

    time_grid = this_time.get('time_grid')

    # -------------------------------------------------------------------------------------------------------------
    # Read grid info from the data files, to pass into info files for this dataset
    # -------------------------------------------------------------------------------------------------------------

    filename, file_extension = os.path.splitext(dhfile)
    filename, file_extension = os.path.splitext(filename)
    info_filename = filename + '.info'
    sinfo = SurfacefitInfo(info_filename)
    if sinfo.key_error:
        log.error('.info file error : {}'.format(sinfo.key_error))
        sys.exit('.info file error : {}'.format(sinfo.key_error))

    # -------------------------------------------------------------------------------------------------------------
    # If a GIA model is given, read it in. Set up grid (only needed to calculate GIA)
    # -------------------------------------------------------------------------------------------------------------

    if gia_model:
        gia = GIAs(gia_model)

        grid = GridArea(sinfo.grid_name, sinfo.grid_binsize)

    # -------------------------------------------------------------------------------------------------------------
    # Find time range covered by data, calculate epoch limits. The times are given in digital years since 1991.0.
    # If epoch start and/or end are given, use them, otherwise use 1991.0 and time of last datapoint respectively.
    # Always round start time down to previous midnight, and end time up to following midnight.
    # May need to adjust epoch_end to avoid truncating the last epoch.
    # Also warn user if last epoch only contains data from the beginning of its timespan.
    # -------------------------------------------------------------------------------------------------------------

    # find actual start and end times in time grid

    data_mintime = np.min([min(t) for t in time_grid.flatten() if t])
    data_maxtime = np.max([max(t) for t in time_grid.flatten() if t])

    epoch_time = datetime(1991, 1, 1, 0, 0, 0)  # epoch of dataset, not epochs for time-averaging
    epoch_year = 1991

    min_start_year = int(data_mintime) + epoch_year
    if isleap(min_start_year):
        days_of_year = 366. * math.modf(float(data_mintime))[0]
    else:
        days_of_year = 365. * math.modf(float(data_mintime))[0]
    dtstart = datetime(min_start_year, 1, 1, 0, 0, 0) + timedelta(days=days_of_year)

    max_start_year = int(data_maxtime) + epoch_year
    if isleap(max_start_year):
        days_of_year = 366. * math.modf(float(data_maxtime))[0]
    else:
        days_of_year = 365. * math.modf(float(data_maxtime))[0]
    dtend = datetime(max_start_year, 1, 1, 0, 0, 0) + timedelta(days=days_of_year)

    log.info('Data start time: {}'.format(dtstart))
    log.info('Data end time: {}'.format(dtend))
    print('Data start time: ', dtstart)
    print('Data end time: ', dtend)

    # if user has supplied their own start time, read in and round it, otherwise use the default

    # Check if epoch_start is given in DD/MM/YYYY format
    if '/' in epoch_start:
        epoch_start_list = epoch_start.split('/')
        if len(epoch_start_list) != 3:
            log.error('--epoch_start must be either decimal year YYYY.yy or dd/mm/yyyy')
            sys.exit('--epoch_start must be either decimal year YYYY.yy or dd/mm/yyyy')
        epoch_start_day = int(epoch_start_list[0])
        epoch_start_month = int(epoch_start_list[1])
        epoch_start_year = int(epoch_start_list[2])
        if epoch_start_day not in range(1, 32):
            log.error('--epoch_start dd/mm/yyyy, dd must be in range 1-31')
            sys.exit('--epoch_start dd/mm/yyyy, dd must be in range 1-31')
        if epoch_start_month not in range(1, 13):
            log.error('--epoch_start dd/mm/yyyy, mm must be in range 1-12')
            sys.exit('--epoch_start dd/mm/yyyy, mm must be in range 1-12')
        if epoch_start_year < 1991 or epoch_start_year > datetime.today().year:
            log.error('--epoch_start dd/mm/yyyy, yyyy must be in range 1990-{}'.format(datetime.today().year))
            sys.exit('--epoch_start dd/mm/yyyy, yyyy must be in range 1990-{}'.format(datetime.today().year))
        epoch_start_dt = datetime(epoch_start_year, epoch_start_month, epoch_start_day, 0, 0, 0)
    else:
        # epoch_start is given in decimal years
        epoch_start_decimal_years = float(epoch_start)
        epoch_start_year = int(epoch_start_decimal_years)
        if isleap(epoch_start_year):
            days_of_year = 366. * math.modf(float(epoch_start))[0]
        else:
            days_of_year = 365. * math.modf(float(epoch_start))[0]
        this_start_dt = datetime(epoch_start_year, 1, 1, 0, 0, 0) + timedelta(days=days_of_year)
        epoch_start_dt = this_start_dt.replace(microsecond=0, second=0, minute=0, hour=0)  # rounded down
    epoch_start_days = (epoch_start_dt - datetime(epoch_start_dt.year, 1, 1))
    if isleap(epoch_start_dt.year):
        fractional_year = float(epoch_start_days.days) / 366.0
    else:
        fractional_year = float(epoch_start_days.days) / 365.0
    epoch_start = float(epoch_start_dt.year - epoch_year) + fractional_year

    # if user has supplied their own end time, read in, otherwise use that found above
    # no need to round it, the epoch limits calculation will do that

    if epoch_end:
        # Check if epoch_end is given in DD/MM/YYYY format
        if '/' in epoch_end:
            epoch_end_list = epoch_end.split('/')
            if len(epoch_end_list) != 3:
                log.error('--epoch_end must be either decimal year YYYY.yy or dd/mm/yyyy')
                sys.exit('--epoch_end must be either decimal year YYYY.yy or dd/mm/yyyy')
            epoch_end_day = int(epoch_end_list[0])
            epoch_end_month = int(epoch_end_list[1])
            epoch_end_year = int(epoch_end_list[2])
            if epoch_end_day not in range(1, 32):
                log.error('--epoch_end dd/mm/yyyy, dd must be in range 1-31')
                sys.exit('--epoch_end dd/mm/yyyy, dd must be in range 1-31')
            if epoch_end_month not in range(1, 13):
                log.error('--epoch_end dd/mm/yyyy, mm must be in range 1-12')
                sys.exit('--epoch_end dd/mm/yyyy, mm must be in range 1-12')
            if epoch_end_year < 1991 or epoch_end_year > datetime.today().year:
                log.error('--epoch_end dd/mm/yyyy, yyyy must be in range 1990-{}'.format(datetime.today().year))
                sys.exit('--epoch_end dd/mm/yyyy, yyyy must be in range 1990-{}'.format(datetime.today().year))
            epoch_end_dt = datetime(epoch_end_year, epoch_end_month, epoch_end_day, 0, 0, 0)
        else:
            # epoch_end is given in decimal years
            epoch_end_decimal_years = float(epoch_end)
            epoch_end_year = int(epoch_end_decimal_years)
            if isleap(epoch_end_year):
                days_of_year = 366. * math.modf(float(epoch_end))[0]
            else:
                days_of_year = 365. * math.modf(float(epoch_end))[0]
            epoch_end_dt = datetime(epoch_end_year, 1, 1, 0, 0, 0) + timedelta(days=days_of_year)
        epoch_end_days = (epoch_end_dt - datetime(epoch_end_dt.year, 1, 1))
        if isleap(epoch_end_dt.year):
            fractional_year = float(epoch_end_days.days) / 366.0
        else:
            fractional_year = float(epoch_end_days.days) / 365.0
        epoch_end = float(epoch_end_dt.year - epoch_year) + fractional_year
    else:
        epoch_end = data_maxtime  # in fractional years since 1991.0
        epoch_end_dt = dtend

    log.info('Requested start time: {}'.format(epoch_start_dt))
    log.info('Requested end time: {}'.format(epoch_end_dt))
    print('Requested start time: ', epoch_start_dt)
    print('Requested end time: ', epoch_end_dt)

    # calculate epoch limits in seconds since CPOM system epoch (ie 1991.0), and extend end time
    # if necessary to avoid truncation

    epoch_length = float(epoch_length)
    epoch_period = epoch_end_dt - epoch_start_dt
    num_days_in_period = float(epoch_period.days)

    num_epochs = math.ceil(num_days_in_period / epoch_length)

    epoch_lo_dt = np.arange(epoch_start_dt, epoch_end_dt, timedelta(days=epoch_length)).astype(datetime)
    epoch_hi_dt = epoch_lo_dt + timedelta(days=epoch_length)

    epoch_lo = np.full(num_epochs, np.nan)
    for i in np.arange(0, num_epochs):
        this_year = epoch_lo_dt[i].year
        if isleap(this_year):
            fractional_year = float((epoch_lo_dt[i] - datetime(this_year, 1, 1)).days) / 366.0
        else:
            fractional_year = float((epoch_lo_dt[i] - datetime(this_year, 1, 1)).days) / 365.0
        epoch_lo[i] = float(this_year - epoch_year) + fractional_year

    epoch_hi = np.full(num_epochs, np.nan)
    for i in np.arange(0, num_epochs):
        this_year = epoch_hi_dt[i].year
        if isleap(this_year):
            fractional_year = float((epoch_hi_dt[i] - datetime(this_year, 1, 1)).days) / 366.0
        else:
            fractional_year = float((epoch_hi_dt[i] - datetime(this_year, 1, 1)).days) / 365.0
        epoch_hi[i] = float(this_year - epoch_year) + fractional_year

    if epoch_hi.max() != epoch_end:
        epoch_end = epoch_hi.max()
        epoch_end_dt = epoch_hi_dt.max()

    log.info('End time after last epoch allowed to complete in full: {}'.format(epoch_end_dt))
    print('End time after last epoch allowed to complete in full: ', epoch_end_dt)

    # -------------------------------------------------------------------------------------------------------------
    # The default epoch start will make sure that all missions use the same, regular set of epochs where
    # they overlap (given the same epoch length), but will introduce a lot of empty epochs. Using the
    # dataset time range, remove any epochs which cannot contain data.
    # -------------------------------------------------------------------------------------------------------------

    usable_epochs = np.where(np.logical_and(epoch_hi > data_mintime, epoch_lo < data_maxtime))
    if np.any(usable_epochs):
        epoch_lo = epoch_lo[usable_epochs]
        epoch_hi = epoch_hi[usable_epochs]
        epoch_lo_dt = epoch_lo_dt[usable_epochs]
        epoch_hi_dt = epoch_hi_dt[usable_epochs]
        epoch_start_dt = epoch_lo_dt.min()
        epoch_end_dt = epoch_hi_dt.max()
        num_epochs = len(usable_epochs[0])
    else:
        log.error('No data found within epoch limits')
        sys.exit('No data found within epoch limits')

    log.info('After removal of empty epochs, final epoch summary: ')
    log.info('  Number of epochs: {}'.format(num_epochs))
    log.info('  Epoch start: {}'.format(epoch_start_dt))
    log.info('  Epoch end: {}'.format(epoch_end_dt))
    print('After removal of empty epochs, final epoch summary: ')
    print('  Number of epochs: ', num_epochs)
    print('  Epoch start: ', epoch_start_dt)
    print('  Epoch end: ', epoch_end_dt)

    # -------------------------------------------------------------------------------------------------------------
    # Get grid dimensions from the input dh array
    # -------------------------------------------------------------------------------------------------------------

    ncols = np.shape(dh_grid)[1]
    nrows = np.shape(dh_grid)[0]

    # -------------------------------------------------------------------------------------------------------------
    # Create output 3d grid for robust averaged dh, its std dev, and number of datapoints used.
    # The start and end epochs may contain data from a time-range less than that of the full epoch, so
    # for each cell/epoch save the min and max times covered - these can be used to filter epochs.
    # -------------------------------------------------------------------------------------------------------------

    dh_ave = np.full((num_epochs, nrows, ncols), np.nan)
    input_dh_dens = np.zeros((num_epochs, nrows, ncols))
    input_dh_stddev = np.full((num_epochs, nrows, ncols), np.nan)
    input_dh_start_time = np.full((num_epochs, nrows, ncols), np.nan)
    input_dh_end_time = np.full((num_epochs, nrows, ncols), np.nan)

    # -------------------------------------------------------------------------------------------------------------
    # If GIA correction required, calculate uplift grid
    # -------------------------------------------------------------------------------------------------------------

    if gia_model:
        grid_x, grid_y = np.meshgrid(grid.cell_x_centres, grid.cell_y_centres)
        uplift_grid = gia.interp_gia(grid.crs_bng, grid.crs_wgs, grid_x, grid_y)

    # -------------------------------------------------------------------------------------------------------------
    # Run through all cells and get data where possible. If GIA correction required, put it in before getting stats.
    # Robust stats use sigma-clipping, default sigma=3, maxiters=5. They return mean, median and std dev, where
    # the std dev is by default the population std dev (std_ddof=0), rather than the sample std dev (std_ddof=1).
    # std dev = sqrt ( total (data - mean)^2 / (number of datapoints - std_ddof))
    # Note IDL will default to sample std dev, but here we have all the datapoints we want, they're not a subsample
    # of a larger group, so the population std dev is appropriate.
    # Work out how many grid cells are used, irrespective of number of epochs used in the cell
    # -------------------------------------------------------------------------------------------------------------

    total_cells=nrows*ncols
    current_cell=0
    start = timer()
    for col in range(ncols):
        for row in range(nrows):
            current_cell += 1
            if np.remainder(current_cell, 100000) == 0:
                log.info('Processed {} of {} grid cells in {} '.format((100. * current_cell / total_cells), total_cells, timedelta(seconds=timer()-start)))
                print('Processed {:.1f}% '.format(100. * current_cell / total_cells), ' of ', total_cells, ' grid cells in ', timedelta(seconds=timer()-start))
            if dh_grid[row, col]:
                dh = np.asarray(dh_grid[row, col].tolist())
                time = np.asarray(time_grid[row, col].tolist())
                for i in np.arange(0, num_epochs):
                    ok = np.where(np.logical_and(time >= epoch_lo[i], time < epoch_hi[i]))
                    if np.any(ok):
                        if gia_model:
                            this_min_time = min(time[ok])
                            this_uplift = uplift_grid[row, col]
                            for j in ok: dh[j] = dh[j] - ((time[j] - this_min_time) * this_uplift)
                        this_stats = sigma_clipped_stats(dh[ok])
                        dh_ave[i, row, col] = this_stats[0]
                        input_dh_dens[i, row, col] = len(ok[0])
                        input_dh_stddev[i, row, col] = this_stats[2]
                        input_dh_start_time[i, row, col] = min(time[ok])
                        input_dh_end_time[i, row, col] = max(time[ok])

    cells_and_times_used = np.isfinite(dh_ave)
    cells_used = np.sum(cells_and_times_used, axis=0)
    ok = np.where(cells_used > 0)
    cell_count = len(ok[0])

    end_time = datetime.now()
    log.info('Duration: {}'.format(end_time - start_time))
    print('Duration: {}'.format(end_time - start_time))

    # ----------------------------------------------------------------------------------
    # Save output grid arrays to disk as npz file, using dh filename as guide to output filename
    # ----------------------------------------------------------------------------------

    f_head, f_tail = os.path.split(dhfile)
    f_tail2 = f_tail.replace('surfacefit', 'epoch_average')
    f_tail3 = f_tail2.replace('.dh', '')
    output_filename = outdir + '/' + f_tail3

    log.info('Saving epoch averaged data as {}'.format(output_filename))
    print('Saving epoch averaged data as ', output_filename)
    np.savez(output_filename,
             dh_ave=dh_ave,
             input_dh_dens=input_dh_dens,
             input_dh_stddev=input_dh_stddev,
             input_dh_start_time=input_dh_start_time,
             input_dh_end_time=input_dh_end_time,
             epoch_lo=epoch_lo,
             epoch_hi=epoch_hi,
             epoch_lo_dt=epoch_lo_dt,
             epoch_hi_dt=epoch_hi_dt)

    # ----------------------------------------------------------------------------------
    # Output info as text file, using dh filename as guide to output filename
    # ----------------------------------------------------------------------------------

    f_tail4 = f_tail3.replace('.npz', '.info')
    output_info_filename = outdir + '/' + f_tail4

    log.info('Saving info as {}'.format(output_info_filename))
    print('Saving info as ', output_info_filename)
    with open(output_info_filename, "w") as file:
        file.write('Plot area= {}\n'.format(sinfo.plot_area))
        file.write('---------------------------------------------\n')
        file.write('Grid Parameters\n')
        file.write('---------------------------------------------\n')
        file.write('Mission= {}\n'.format(sinfo.mission.upper()))
        file.write('Grid name= {}\n'.format(sinfo.grid_name))
        file.write('Grid scenario name= {}\n'.format(sinfo.grid_scenario))
        file.write('Grid bin size= {}\n'.format(int(sinfo.grid_binsize)))
        file.write('Grid projection crs= {}\n'.format(sinfo.grid_crs))
        file.write('Grid width= {}\n'.format(int(sinfo.grid_width)))
        file.write('Grid height= {}\n'.format(int(sinfo.grid_height)))
        file.write('Grid x0= {}\n'.format(float(sinfo.grid_x0)))
        file.write('Grid y0= {}\n'.format(float(sinfo.grid_y0)))
        file.write('First cycle= {}\n'.format(int(sinfo.grid_cycle1)))
        file.write('Last cycle= {}\n'.format(int(sinfo.grid_cycle2)))
        file.write('First cycle start date= {}\n'.format(sinfo.grid_cycle1_start_date))
        file.write('Last cycle start date= {}\n'.format(sinfo.grid_cycle2_start_date))
        file.write('grid_for_surface_elevation_change git commit version= {}\n'.format(sinfo.grid_git_commit_version))
        file.write('grid_for_surface_elevation_change git commit date= {}\n'.format(sinfo.grid_git_commit_date))
        if sinfo.mission == 'cs2':
            file.write('LRM Elevation parameter= {}\n'.format(sinfo.grid_elevation_param_lrm))
            file.write('SIN Elevation parameter= {}\n'.format(sinfo.grid_elevation_param_sin))
            file.write('LRM Power parameter= {}\n'.format(sinfo.grid_power_param_lrm))
            file.write('SIN Power parameter= {}\n'.format(sinfo.grid_power_param_sin))
        else:
            file.write('Elevation parameter= {}\n'.format(sinfo.grid_elevation_param))
            file.write('Power parameter= {}\n'.format(sinfo.grid_power_param))
        file.write('Mask name= {}\n'.format(sinfo.grid_mask_name))

        file.write('Number of cells with data in= {}\n'.format(sinfo.grid_num_cells_with_data_in))
        file.write('Number of measurements gridded= {}\n'.format(sinfo.grid_num_measurements_gridded))
        file.write('Number of measurements rejected due to height failure= {}\n'.format(
            sinfo.grid_num_measurements_rejected_height_failure))
        file.write('Number of measurements rejected due to height out of range= {}\n'.format(
            sinfo.grid_num_measurements_rejected_height_out_of_range))
        file.write('Grid git commit version= {}\n'.format(sinfo.grid_git_commit_version))
        file.write('Grid git commit date= {}\n'.format(sinfo.grid_git_commit_date))

        file.write('---------------------------------------------\n')
        file.write('Surface fit tunable parameters used\n')
        file.write('---------------------------------------------\n')
        file.write('Sigma filter= {}\n'.format(sinfo.sf_sigma_filter))
        file.write('Maximum number of surface fit iterations allowed= {}\n'.format(
            sinfo.sf_max_surface_fit_iterations_allowed))
        file.write('Maximum number of linear fit iterations allowed= {}\n'.format(
            sinfo.sf_max_linear_fit_iterations_allowed))
        file.write('Minimum number of measurements in cell for surface fit= {}\n'.format(
            sinfo.sf_min_measurements_in_cell_for_surface_fit))
        file.write('Power correction applied= {}\n'.format(sinfo.sf_power_correction_applied))
        file.write('Power correction length in years= {:.2f}\n'.format(float(sinfo.sf_power_correction_length_years)))
        file.write('Power correction start date= {}\n'.format(sinfo.sf_power_correction_start_date))
        file.write('Power correction end date= {}\n'.format(sinfo.sf_power_correction_end_date))

        file.write('surfacefit Git commit version= {}\n'.format(sinfo.sf_git_commit_version))
        file.write('surfacefit Git commit date= {}\n'.format(sinfo.sf_git_commit_date))
        file.write('---------------------------------------------\n')
        file.write('Surface fit output statistics\n')
        file.write('---------------------------------------------\n')
        file.write('Duration of surfacefit processing= {}\n'.format(sinfo.sf_duration_surface_fit_processing))
        file.write('Start date of surfacefit output= {}\n'.format(sinfo.sf_start_date_surface_fit_output))
        file.write('End date of surfacefit output= {}\n'.format(sinfo.sf_end_date_surface_fit_output))
        file.write('Number of cells fitted= {}\n'.format(sinfo.sf_number_cells_fitted))
        file.write('Number of cells rejected due to time outside min/max= {}\n'.format(
            sinfo.sf_number_cells_rejected_outside_time_minmax))
        file.write('Number of cells rejected due to too few measurements= {}\n'.format(
            sinfo.sf_number_cells_rejected_too_few_measurements))
        file.write('Number of cells rejected due to too few measurements after sigma filter= {}\n'.format(
            sinfo.sf_number_cells_rejected_too_few_measurements_after_sigma_filter))
        file.write('Number of cells rejected due to timespan too short= {}\n'.format(
            sinfo.sf_number_cells_rejected_timespan_too_short))
        file.write('Number of cells rejected due to residual rms exceeded= {}\n'.format(
            sinfo.sf_number_cells_rejected_residual_rms_exceeded))
        file.write('Number of cells rejected due to fit not converging= {}\n'.format(
            sinfo.sf_number_cells_rejected_fit_not_converging))
        file.write('Number of cells rejected due to fit function failure= {}\n'.format(
            sinfo.sf_number_cells_rejected_fit_failure))
        file.write(
            'Less than 4 values left in linear fit= {}\n'.format(sinfo.sf_less_4_values_left_in_linear_fit))
        file.write('Number of cells power corrected= {}\n'.format(sinfo.sf_ncells_power_corrected))

        file.write('---------------------------------------------\n')
        file.write('Epoch averaging output statistics\n')
        file.write('---------------------------------------------\n')
        file.write('GIA model= {}\n'.format(gia_model))
        file.write('Duration of epoch_average processing= {}\n'.format(end_time - start_time))
        file.write('Epoch length in days= {}\n'.format(epoch_length))
        file.write('Number of epochs= {}\n'.format(num_epochs))
        file.write('Epoch averaging start time= {}\n'.format(epoch_start_dt))
        file.write('Epoch averaging end time= {}\n'.format(epoch_end_dt))
        file.write('Number of cells containing averaged data= {}\n'.format(cell_count))
        file.write('---------------------------------------------\n')
        file.write('Software version\n')
        file.write('---------------------------------------------\n')
        file.write('Git commit version= {}\n'.format(git_commit_sha))
        file.write('Git commit date= {} {} {}\n'.format(git_day, git_month, git_year))


    # Test reading .info file
    einfo = EpochaverageInfo(output_info_filename)
    if einfo.key_error:
        sys.exit(einfo.key_error)

    return output_filename


class EpochaverageInfo:
    """
    Class to read .info file generated by epoch_average.py

    usage:

    einfo=EpochaverageInfo('/path/to/surfacefit.info')
    if einfo.key_error:
            print(einfo.key_error)
    else:
                print(einfo.mission)
                ...
        """

    def __init__(self, filename):
        print('Loading ', filename)
        self.filename = filename
        self.dict = {}
        if not self.read():
            self.key_error = 'File not found : {}'.format(filename)
            return None

        self.key_error = None

        try:
            # Plot Parameters
            self.plot_area = self.dict['Plot area'].strip()

            # Mission Parameters
            self.mission = self.dict['Mission'].strip().lower()

            # Grid Parameters
            self.grid_name = self.dict['Grid name'].strip()
            self.grid_scenario = self.dict['Grid scenario name'].strip()
            self.grid_binsize = int(self.dict['Grid bin size'].strip())
            self.grid_crs = self.dict['Grid projection crs'].strip()
            self.grid_width = float(self.dict['Grid width'].strip())
            self.grid_height = float(self.dict['Grid height'].strip())
            self.grid_x0 = float(self.dict['Grid x0'].strip())
            self.grid_y0 = float(self.dict['Grid y0'].strip())

            if 'cs2' in self.mission:
                self.grid_elevation_param_lrm = self.dict['LRM Elevation parameter'].strip()
                self.grid_elevation_param_sin = self.dict['SIN Elevation parameter'].strip()
                self.grid_power_param_lrm = self.dict['LRM Power parameter'].strip()
                self.grid_power_param_sin = self.dict['SIN Power parameter'].strip()
            else:
                self.grid_elevation_param = self.dict['Elevation parameter'].strip()
                self.grid_power_param = self.dict['Power parameter'].strip()

            self.grid_num_cells_with_data_in = int(self.dict['Number of cells with data in'].strip())
            self.grid_num_measurements_gridded = int(self.dict['Number of measurements gridded'])
            self.grid_num_measurements_rejected_height_failure = int(
                self.dict['Number of measurements rejected due to height failure'])
            self.grid_num_measurements_rejected_height_out_of_range = int(
                self.dict['Number of measurements rejected due to height out of range'])

            self.grid_cycle1 = int(self.dict['First cycle'].strip())
            self.grid_cycle2 = int(self.dict['Last cycle'].strip())
            self.grid_cycle1_start_date = self.dict['First cycle start date'].strip()
            self.grid_cycle2_start_date = self.dict['Last cycle start date'].strip()
            self.grid_mask_name = self.dict['Mask name'].strip()
            self.grid_git_commit_version = self.dict['Grid git commit version'].strip()
            self.grid_git_commit_date = self.dict['Grid git commit date'].strip()

            # Surface Fit  Parameters

            self.sf_sigma_filter = float(self.dict['Sigma filter'].strip())
            self.sf_max_surface_fit_iterations_allowed = int(
                self.dict['Maximum number of surface fit iterations allowed'].strip())
            self.sf_max_linear_fit_iterations_allowed = int(
                self.dict['Maximum number of linear fit iterations allowed'].strip())
            self.sf_min_measurements_in_cell_for_surface_fit = int(
                self.dict['Minimum number of measurements in cell for surface fit'].strip())
            self.sf_power_correction_applied = self.dict['Power correction applied'].strip()
            self.sf_power_correction_length_years = self.dict['Power correction length in years'].strip()
            self.sf_power_correction_start_date = self.dict['Power correction start date'].strip()
            self.sf_power_correction_end_date = self.dict['Power correction end date'].strip()

            self.sf_duration_surface_fit_processing = self.dict['Duration of surfacefit processing'].strip()
            self.sf_start_date_surface_fit_output = self.dict['Start date of surfacefit output'].strip()
            self.sf_end_date_surface_fit_output = self.dict['End date of surfacefit output'].strip()
            self.sf_number_cells_fitted = int(self.dict['Number of cells fitted'].strip())
            self.sf_number_cells_rejected_outside_time_minmax = int(
                self.dict['Number of cells rejected due to time outside min/max'].strip())
            self.sf_number_cells_rejected_too_few_measurements = int(
                self.dict['Number of cells rejected due to too few measurements'].strip())
            self.sf_number_cells_rejected_too_few_measurements_after_sigma_filter = int(
                self.dict['Number of cells rejected due to too few measurements after sigma filter'].strip())
            self.sf_number_cells_rejected_timespan_too_short = int(
                self.dict['Number of cells rejected due to timespan too short'].strip())
            self.sf_number_cells_rejected_residual_rms_exceeded = int(
                self.dict['Number of cells rejected due to residual rms exceeded'].strip())
            self.sf_number_cells_rejected_fit_not_converging = int(
                self.dict['Number of cells rejected due to fit not converging'].strip())
            self.sf_number_cells_rejected_fit_failure = int(
                self.dict['Number of cells rejected due to fit function failure'].strip())
            self.sf_less_4_values_left_in_linear_fit = int(self.dict['Less than 4 values left in linear fit'].strip())
            self.sf_ncells_power_corrected = int(self.dict['Number of cells power corrected'].strip())
            self.sf_git_commit_version = self.dict['Git commit version'].strip()
            self.sf_git_commit_date = self.dict['Git commit date'].strip()

            # Epoch Average Parameters

            self.ea_gia_model = self.dict['GIA model'].strip()
            self.ea_duration_epoch_average_processing = self.dict['Duration of epoch_average processing'].strip()
            self.ea_epoch_length_in_days = self.dict['Epoch length in days'].strip()
            self.ea_number_of_epochs = self.dict['Number of epochs'].strip()
            self.ea_epoch_averaging_start_time = self.dict['Epoch averaging start time'].strip()
            self.ea_epoch_averaging_end_time = self.dict['Epoch averaging end time'].strip()
            self.ea_number_of_cells_containing_averaged_data = self.dict[
                'Number of cells containing averaged data'].strip()

            # This Software Version
            self.git_commit_version = self.dict['Git commit version'].strip()
            self.git_commit_date = self.dict['Git commit date'].strip()

        except KeyError as err:
            log.error('KeyError: {}'.format(err))
            sys.exit('KeyError: {}'.format(err))
            self.key_error = err

    def read(self):
        """
        Read the .info file stored with each gridded data set's .npz file
        :param filename: path of grid ... .info file
        :return: dictionary with files keywords and values, or None if filename is noty readable
        """

        if not os.path.isfile(self.filename):
            print('Grid info file not found : ', self.filename)
            return False

        with open(self.filename, 'r') as file:
            info_txt = file.readlines()

            self.dict = {}

            # parse .info file text
            for line in info_txt:
                if '=' in line:
                    words = line.split('=')
                    self.dict[words[0]] = (words[1]).rstrip()
        return True

    def print(self):
        """
        Print self out neatly
        """

        print('Filename :', self.filename)
        for key in self.dict:
            print(key, self.dict[key])

    def compare(self, gridinfo):
        """
        compare fields from another gridinfo object, or other object containing grid info eg surfacefitinfo
        :param gridinfo:
        :return:
        """
        if gridinfo.mission != self.mission: return False
        if gridinfo.grid_name != self.grid_name: return False
        if gridinfo.grid_crs != self.grid_crs: return False
        if gridinfo.grid_binsize != self.grid_binsize: return False
        if gridinfo.grid_width != self.grid_width: return False
        if gridinfo.grid_height != self.grid_height: return False

        return True


# ----------------------------------------------------------------------------------------------------------
#  Main  Code
# ----------------------------------------------------------------------------------------------------------
if __name__ == '__main__':

    import sys
    import argparse

    # initiate the parser
    parser = argparse.ArgumentParser()

    # add long and short argument
    parser.add_argument("--griddir", "-g",
                        help="Directory containing dh and time single mission grid files in .npz format, generated by surfacefit.py. If this is given, the dh and time files in this directory will be used, so do not provide filenames.")
    parser.add_argument("--dhfile", "-f1",
                        help="Single mission dh grid file in .npz format, generated by surfacefit.py. Only provide a filename if you have NOT specified a directory.")
    parser.add_argument("--timefile", "-f2",
                        help="Single mission time grid file in .npz format, generated by surfacefit.py. Only provide a filename if you have NOT specified a directory.")
    parser.add_argument("--outdir", "-o",
                        help="Path of output directory for output files. If not specified will use directory path of input grid file for output")
    parser.add_argument("--epoch_length", "-e", help="(integer, default=30), length of averaging epoch, in days")
    parser.add_argument("--epoch_start", "-t1",
                        help="(optional) start time of first epoch in decimal years YYYY.yy or DD/MM/YYYY format. If not provided then 1991.0 is used. Will be rounded down to start of day.")
    parser.add_argument("--epoch_end", "-t2",
                        help="(optional) end time of last epoch in decimal years YYYY.yy or DD/MM/YYYY format. If not provided then latest time in grid is used. If this time would truncate last epoch, will be extended to avoid truncation. Will be rounded up to end of day.")
    parser.add_argument("--gia_model", "-gia",
                        help="(optional) GIA model to use for correction. If not provided, no correction will be applied. List model names with gia_models.py --list_all.")

    if len(sys.argv) == 1:
        print('no args provided')
        parser.print_help()
        sys.exit()

    # read arguments from the command line or from the args parameter

    args = parser.parse_args()

    epoch_average(griddir=args.griddir,
                  dhfile=args.dhfile,
                  timefile=args.timefile,
                  outdir=args.outdir,
                  epoch_length=args.epoch_length,
                  epoch_start=args.epoch_start,
                  epoch_end=args.epoch_end,
                  gia_model=args.gia_model)